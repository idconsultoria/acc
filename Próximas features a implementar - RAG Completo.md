# Pr√≥ximas features a implementar ‚Äî RAG Completo

## Issue 1 ‚Äî Chunking Inteligente e Extra√ß√£o de Metadados

**Objetivo**
- Evoluir o pipeline de ingest√£o (`backend/app/domain/artifacts/workflows.py`) para produzir chunks semanticamente coerentes, com metadados estruturais persistidos e dispon√≠veis para busca.

**Contexto atual**
- `chunk_text` usa janelas fixas de caracteres e n√£o entende hierarquia de documentos.
- `ArtifactChunk` armazena apenas `content` e `embedding`; os reposit√≥rios (`artifacts_repo`, `knowledge_repo`) n√£o suportam metadados.
- A UI de artefatos em `frontend/src/views/AdminView.tsx` n√£o exp√µe nenhuma vis√£o granular dos chunks.

**Plano de implementa√ß√£o**
- *Modelagem de dom√≠nio*
  - Introduzir um novo value object `ChunkMetadata` em `backend/app/domain/artifacts/types.py` contendo campos como `section_title`, `section_level`, `content_type`, `position`, `token_count` e `breadcrumbs` (lista de t√≠tulos ancestrais).
  - Estender `ArtifactChunk` para incluir `metadata: ChunkMetadata | None` preservando compatibilidade retroativa (metadados opcionais quando n√£o existentes).
- *Chunking adaptativo*
  - Criar m√≥dulo `backend/app/infrastructure/files/structured_chunker.py` com fun√ß√µes: `analyze_structure(text: str) -> list[StructuredBlock]` que identifica headings Markdown, listas numeradas e par√°grafos, e `generate_chunks(blocks, max_tokens, overlap_tokens)` que combina blocos mantendo contexto.
  - Atualizar `create_artifact_from_text`/`create_artifact_from_pdf` para usar o novo chunker. Computar tokens via `tiktoken` (adicionar a depend√™ncia no `backend/requirements.txt`) ou fallback simples baseado em n√∫mero de palavras se a lib n√£o estiver dispon√≠vel.
  - Implementar sobreposi√ß√£o inteligente: reutilizar `position` e garantir que o conte√∫do concatenado inclua overlap via neighbors list.
- *Extra√ß√£o de metadados durante parsing*
  - Para PDFs, reabilitar `PDFProcessor.extract_text` com uso opcional de `pymupdf` quando dispon√≠vel, mas garantir fallback para texto plano. Adicionar m√©todo `extract_with_metadata` que devolve pares `(text, attrs)` se o PDF contiver bookmarks.
  - Enriquecer cada chunk com: t√≠tulo da se√ß√£o mais pr√≥xima, n√≠vel hier√°rquico (H1, H2 etc.), tipo (`paragraph`, `bullet`, `quote`, `table`), ordem global e contagem de tokens.
- *Persist√™ncia*
  - Adicionar colunas `section_title`, `section_level`, `content_type`, `position`, `token_count`, `breadcrumbs` (JSON) em `artifact_chunks` por nova migration SQL (seguir padr√£o de `backend/database/migrations`). Atualizar `schema.sql` e migrations auxiliares para Supabase.
  - Atualizar `ArtifactsRepository.save`, `find_by_id`, `save_chunks`, `update_artifact_content` para gravar/carregar `metadata`. Ajustar o tipo de retorno em `KnowledgeRepository` para popular `ArtifactChunk.metadata`.
- *API & UI*
  - Estender DTOs em `backend/app/api/dto.py` e rotas de artefatos para devolver metadados junto ao conte√∫do.
  - Adicionar na view e no modal de artefatos a op√ß√£o de visualizar o texto ou PDF original na √≠ntegra e n√£o remontado dos chunks (verificar altera√ß√µes necess√°rias no database/backend)
  - Na UI admin (`AdminView`), adicionar drawer modal que liste chunks com se√ß√£o, tipo e snippet, permitindo inspe√ß√£o da segmenta√ß√£o.
  - Exibir breadcrumbs/metadados na visualiza√ß√£o de cita√ß√µes (`frontend/src/components/shared/SourceCitation.tsx`).
- *Testes*
  - Cobrir `structured_chunker` com casos de headings aninhados e listas.
  - Ajustar testes em `backend/tests/test_domain_workflows.py` para validar gera√ß√£o de metadados e estabilidade do n√∫mero de chunks.
  - Criar testes E2E simulando ingest√£o de texto e validando resposta da API (`tests/test_api_routes.py`).

**Depend√™ncias e riscos**
- Necessidade de sincronizar migrations com Supabase (executar manualmente ap√≥s deploy).
- Tokerniza√ß√£o depende de biblioteca externa; definir fallback claro.
- Volume adicional de metadados aumenta tamanho da janela de contexto ‚Äî mitigado pela Issue 3.

## Issue 2 ‚Äî Prompt Engineering Profissional

**Objetivo**
- Organizar prompts em templates versionados, inserir exemplos (few-shot), formatos diferenciados para artefatos/aprendizados e embutir metaprompt de autoavalia√ß√£o antes da resposta final.

**Contexto atual**
- `GeminiService.generate_advice` monta string monol√≠tica com regras fixas e sem exemplos.
- N√£o h√° separa√ß√£o entre system prompt, context prompt e user prompt; aprendizados n√£o t√™m destaque.
- N√£o existe mecanismo de auto-reflex√£o ou checagem de cita√ß√µes.

**Plano de implementa√ß√£o**
- *Arquitetura de prompts*
  - Criar m√≥dulo `backend/app/domain/agent/prompt_templates.py` com classes `PromptSection` e `PromptTemplate`. Incluir template base com placeholders para instru√ß√£o, artefatos, aprendizados, hist√≥rico, consulta e instru√ß√µes de metaavalia√ß√£o.
  - Armazenar exemplos few-shot em JSON (`backend/app/domain/agent/prompt_examples.py`) com estrutura `{user, agent, cited_sources}` para reuso.
  - Definir `PromptRenderer` respons√°vel por aplicar formata√ß√£o Markdown (ex: `### Fonte 1 ‚Äî {section_title}`) e por incluir aprendizados com destaque (blockquote, emoji, etc.).
- *Integra√ß√£o com LLM*
  - Refatorar `GeminiService.generate_advice` para usar o builder: `PromptTemplate.build_prompt(...)`. Utilizar streaming nativo (prepara√ß√£o para Issue 4) e separar system prompt (persona + regras), context prompt (artefatos/aprendizados formatados) e user prompt.
  - Inserir few-shots no array `contents` da API Gemini (`model.generate_content` com `messages=[system, example1_user, example1_model, ..., user]`).
  - Acrescentar metaprompt: ap√≥s gerar a primeira resposta, executar chamada curta `generate_content` com prompt de auto-checagem que valide ader√™ncia √†s fontes e sinalize problemas; se houver ajustes, reescrever resposta final destacando corre√ß√µes.
- *Formata√ß√£o especial*
  - Implementar fun√ß√£o `format_artifact_chunk(chunk)` que inclui t√≠tulo, tipo, resumo (primeiras N frases) e ID da fonte.
  - Diferenciar aprendizados: agrupar por peso (Issue 5) e prefixar com `üß† Insight Relevante`.
- *Persist√™ncia / Configura√ß√£o*
  - Permitir versionamento do prompt (coluna `prompt_version` em `agent_settings`). Atualizar `AgentSettingsRepository` para salvar vers√£o corrente e expor via API.
- *Testes*
  - Criar testes unit√°rios para `PromptTemplate` validando presen√ßa de se√ß√µes e placeholders.
  - Adicionar teste de integra√ß√£o (mock Gemini) verificando que `generate_advice` envia mensagens com few-shots e aplica metaprompt.
- *Documenta√ß√£o*
  - Atualizar `design/5_guia_implementacao_frontend.md` com novo contrato de system prompt.

**Depend√™ncias e riscos**
- Chamadas adicionais ao Gemini (auto-reflex√£o) aumentam lat√™ncia e custo; configurar flag `ENABLE_SELF_REFLECTION` para desligar quando necess√°rio.
- Precisamos confirmar limites de tokens do modelo para acomodar few-shots + contexto.

## Issue 3 ‚Äî Gest√£o de Janela de Contexto

**Objetivo**
- Permitir fixar artefatos/aprendizados na janela, visualizar contexto ativo e gerenciar or√ßamento de tokens por conversa, persistindo prefer√™ncias do usu√°rio.

**Contexto atual**
- `continue_conversation` recebe todos os chunks retornados pela busca sem distin√ß√£o.
- N√£o existe UI para o usu√°rio priorizar fontes.
- As configura√ß√µes por conversa n√£o s√£o persistidas (`ConversationsRepository` s√≥ grava mensagens/t√≥pico).

**Plano de implementa√ß√£o**
- *Modelagem*
  - Criar entidade `ContextSlot` com campos `conversation_id`, `item_type` (`CHUNK` | `LEARNING`), `item_id`, `is_pinned`, `manual_weight`, `created_at`.
  - Adicionar tabela `conversation_context_slots` via migration PostgreSQL e Supabase.
- *Backend*
  - Implementar workflow `manage_context_window` em `backend/app/domain/conversations/workflows.py` (ou novo m√≥dulo) que receba: chunks candidatos, aprendizados, slots persistidos e or√ßamento de tokens. Fun√ß√£o deve ordenar itens (`pinned` > `re-ranking score`) e retornar subconjunto dentro do limite.
  - Atualizar `KnowledgeRepository.find_relevant_knowledge` para aceitar par√¢metro opcional `exclude_ids` (evitar duplicar itens j√° pinados) e retornar score bruto para combina√ß√£o.
  - Adicionar novo reposit√≥rio `ConversationContextRepository` em `backend/app/infrastructure/persistence` para CRUD dos slots.
  - Expor rotas REST (`/conversations/{id}/context`) em `backend/app/api/routes/conversations.py` para listar itens ativos, fixar/desfixar, definir pesos manuais e atualizar or√ßamento m√°ximo (armazenar em nova coluna `context_token_budget` na tabela `conversations`).
  - Integrar `continue_conversation`: antes da chamada ao LLM, compor contexto com `ContextManager`, garantindo que itens pinados entrem primeiro e registrando o conjunto usado (salvar log em `conversation_context_slots` ou tabela auxiliar `context_usage_history`).
- *Frontend*
  - Criar componente `ContextPanel` exibido em `ChatView` (coluna lateral ou drawer) com: lista de artefatos relevantes, indicadores de token, bot√µes de `Pin`/`Unpin`, slider para or√ßamento de tokens e reorder por drag-and-drop (biblioteca `@dnd-kit/core`).
  - Atualizar `api/client.ts` com novos m√©todos (`getConversationContext`, `pinContextItem`, `updateContextBudget`).
- *Persist√™ncia cliente*
  - `zustand` store (`frontend/src/state/store.ts`) precisa guardar `contextSettings` por conversa para resposta otimista.
- *Testes*
  - Novos testes backend validando prioriza√ß√£o de slots e respeito ao or√ßamento.
  - Testes de integra√ß√£o na API (mock) assegurando que pinar um item reflete na pr√≥xima resposta.
  - Testes de interface (React Testing Library) para `ContextPanel`.

**Depend√™ncias e riscos**
- UI mais complexa aumenta superf√≠cie de estados; garantir loaders e mensagens quando SSE (Issue 4) estiver ativo.
- Precisamos controlar concorr√™ncia: m√∫ltiplos clientes alterando o mesmo contexto simultaneamente.

## Issue 4 ‚Äî Streaming de Respostas + Visualiza√ß√£o de Etapas

**Objetivo**
- Entregar respostas em tempo real (token por token) e expor progresso das fases do RAG no backend e frontend.

**Contexto atual**
- A rota `POST /conversations/{id}/messages` aguarda todo o processamento e retorna uma √∫nica `MessageDTO`.
- `ChatView` simula typing indicator com estado local, sem eventos de backend.
- `GeminiService` usa `generate_content` s√≠ncrono.

**Plano de implementa√ß√£o**
- *Backend ‚Äî SSE*
  - Adicionar depend√™ncia `sse-starlette`. Criar rota `POST /conversations/{id}/messages/stream` que retorne `EventSourceResponse` emitindo eventos:
    1. `phase:start` (payload com fases: `embedding`, `retrieval`, `prompt_build`, `llm_stream`, `post_process`).
    2. `phase:update` com percentuais e metadados (ex: n√∫mero de chunks selecionados).
    3. `token` emitido a cada fragmento do LLM.
    4. `phase:complete` para cada etapa.
    5. `message:complete` com payload final (mensagem, cita√ß√µes, IDs).
  - Refatorar `continue_conversation` para aceitar callback ass√≠ncrono de progresso (`ProgressEmitter`) e retornar gerador ass√≠ncrono quando streaming estiver habilitado. Manter vers√£o atual para compatibilidade.
  - Atualizar `GeminiService` para usar `self.model.generate_content_async(..., stream=True)`, iterar sobre `response` e emitir tokens via callback. Realizar buffering para reconstruir texto final.
  - Garantir persist√™ncia incremental: salvar mensagens somente ap√≥s evento `message:complete` para evitar registros parciais.
- *Frontend ‚Äî consumo de SSE*
  - Adicionar utilit√°rio `createEventSource` em `frontend/src/api/client.ts` que usa `EventSource` (browser) ou `fetch` com `ReadableStream` (fallback).
  - Em `ChatView`, criar estado `streamingMessage` atualizado com tokens recebidos. Substituir `useMutation` atual por hook custom `useStreamedMessage` que abre SSE, lida com reconex√µes e normaliza eventos.
  - Implementar componente `RagPipelineTimeline` (cards horizontais) mostrando status de cada fase com base nos eventos `phase:*`.
  - Permitir que usu√°rio acompanhe progress√£o no chat (exibir tokens gradualmente no bal√£o do agente).
- *Observabilidade*
  - Acrescentar logs estruturados (JSON) por fase em `GeminiService` e `KnowledgeRepository` para facilitar troubleshooting (incluindo tempos).
- *Testes*
  - Testes unit√°rios simulando `ProgressEmitter` (mocks) para garantir ordem correta dos eventos.
  - Teste end-to-end com `pytest` usando `asyncio` e `httpx.AsyncClient` validando SSE.
  - Testes front-end (Cypress ou Playwright) para garantir que timeline reage aos eventos.

**Depend√™ncias e riscos**
- Streaming do Gemini requer quota/comportamento espec√≠fico; validar limites e timeouts.
- SSE n√£o √© suportado por todos os ambientes serverless; avaliar se Cloud Run/Supabase Edge suportam e documentar fallback (exibir mensagem se streaming desabilitado).

## Issue 5 ‚Äî Sistema de Aprendizados Aprimorado

**Objetivo**
- Refinar a gest√£o dos aprendizados aprovados, com pesos din√¢micos (rec√™ncia, relev√¢ncia), formata√ß√£o no prompt e ferramentas de administra√ß√£o para merge/deduplica√ß√£o.

**Contexto atual**
- `synthesize_learning_from_feedback` salva aprendizados sem pesos.
- `KnowledgeRepository` retorna os tr√™s mais pr√≥ximos apenas por similaridade de embedding.
- N√£o h√° interface dedicada para revisar/mesclar aprendizados; UI depende da aba de feedbacks.

**Plano de implementa√ß√£o**
- *Modelagem*
  - Estender `Learning` com campos opcionais `relevance_weight: float` e `last_used_at: datetime`. Ajustar dataclass e migrations (`learnings` table).
  - Criar entidade `LearningMergeCandidate` para facilitar deduplica√ß√£o.
- *Backend*
  - Atualizar `LearningsRepository` para persistir novos campos e expor m√©todos `update_weights` e `merge(learnings_ids, merged_content)` (executa soft-delete + cria√ß√£o de novo registro).
  - Em `synthesize_learning_from_feedback`, calcular peso inicial com base em tipo de feedback (`POSITIVE` vs `NEGATIVE`) e similaridade com aprendizados existentes (reuso do re-ranking da Issue 6).
  - Implementar servi√ßo `LearningWeighter` que periodicamente (cron/worker) recalcula pesos usando f√≥rmula: `weight = base + recency_decay + feedback_signal`. Para esta fase, expor endpoint manual `/learnings/recalculate` protegido via API Key.
  - Atualizar `KnowledgeRepository.find_relevant_knowledge` para ordenar aprendizados pela pontua√ß√£o combinada (embedding + `relevance_weight`) e atualizar `last_used_at` ap√≥s uso.
- *Frontend Admin*
  - Criar nova tela `AdminLearningsView.tsx` (link no `AdminSidebar`) listando aprendizados com peso, data, fonte. Incluir bot√µes para `Editar`, `Mesclar`, `Remover duplicados`.
  - Implementar modais para merge: selecionar m√∫ltiplos aprendizados, editar texto resultante e enviar para endpoint `/learnings/merge`.
  - Ajustar `PromptTemplate` (Issue 2) para exibir aprendizados ordenados por peso e com formata√ß√£o diferenciada.
- *Deduplica√ß√£o*
  - Adicionar endpoint `POST /learnings/deduplicate` que usa embeddings + similaridade coseno > threshold para sugerir merges; retornar candidatos para UI confirmar.
- *Testes*
  - Expandir `backend/tests/test_domain_workflows.py` com cen√°rios de pesos.
  - Testes para `LearningsRepository` garantindo que merges preservem `source_feedback_id` em hist√≥rico (armazenar em tabela `learning_merge_history`).
  - Tests React para `AdminLearningsView` (renderiza√ß√£o e a√ß√µes principais).

**Depend√™ncias e riscos**
- Ajustar pol√≠ticas de Supabase para permitir updates de colunas adicionais.
- Merge manual exige auditoria; manter hist√≥rico e permitir rollback.

## Issue 6 ‚Äî Melhorias de Qualidade do RAG

**Objetivo**
- Aumentar precis√£o das respostas via re-ranking, cita√ß√µes corretas e estrat√©gia de fallback quando a busca vetorial n√£o retornar contexto √∫til.

**Contexto atual**
- `KnowledgeRepository` retorna top-N por similaridade bruta sem re-ranking contextual.
- `GeminiService` assume que todos os chunks retornados foram usados e cita tudo indiscriminadamente.
- N√£o h√° fallback quando a busca falha; modelo responde sem refer√™ncias.

**Plano de implementa√ß√£o**
- *Re-ranking*
  - Introduzir servi√ßo `CrossEncoderReRanker` (`backend/app/infrastructure/ai/reranker.py`) usando modelo leve (ex: `voyageai` ou `sentence-transformers` via API). Interface `ReRanker.score(query, candidates)`.
  - Atualizar `KnowledgeRepository` para chamar re-ranker ap√≥s busca vetorial, combinando score de similaridade com score cross-encoder. Retornar campo `relevance_score` em `RelevantKnowledge` para uso posterior.
- *Cita√ß√£o inteligente*
  - Modificar `GeminiService.generate_advice` para acompanhar `chunk_index` em tempo real: durante streaming, analisar tokens e procurar padr√£o `Fonte X`. Ap√≥s completar resposta, mapear marcadores √†s fontes corretas (usando regex e ordem de men√ß√£o). Retornar apenas chunks realmente citados.
  - Atualizar `continue_conversation` para preencher `CitedSource.title` com t√≠tulo real do artefato (usar `ArtifactsRepository.get_artifact_data`).
  - No front, alterar `SourceCitation` para exibir n√∫mero da fonte e tooltip com se√ß√£o/tipo (Issue 1).
- *Fallback strategy*
  - Implementar verifica√ß√£o de confian√ßa: se `relevance_score` m√©dio < threshold ou n√£o houver chunks, enviar prompt alternativo para Gemini enfatizando que n√£o h√° fontes suficientes e solicitando resposta gen√©rica com disclaimers.
  - Registrar evento `knowledge:fallback` em logs e retornar flag `used_fallback` para exibirmos aviso na UI.
  - Na UI (`ChatView`), se fallback for usado, mostrar `Alert` informando que n√£o houve fontes citadas.
- *Testes*
  - Criar casos de teste para re-ranking (mock re-ranker) garantindo ordena√ß√£o correta.
  - Testar cita√ß√£o inteligente com resposta que n√£o cita todos os chunks.
  - Testar fallback (mock `KnowledgeRepository` retornando vazio) e validar mensagem de alerta no front.

**Depend√™ncias e riscos**
- Re-ranker externo pode aumentar lat√™ncia; prever cache na aplica√ß√£o e op√ß√£o de desativa√ß√£o (`settings_repo`).
- Parsing de cita√ß√µes precisa ser robusto a varia√ß√µes (usar regex flex√≠vel e fallback manual).


