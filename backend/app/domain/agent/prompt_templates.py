"""Infraestrutura de templates e renderiza√ß√£o de prompts para o agente."""
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Sequence
import re
from datetime import datetime

from app.domain.agent.types import AgentInstruction
from app.domain.agent.prompt_examples import PromptExample
from app.domain.artifacts.types import ArtifactChunk
from app.domain.conversations.types import Message, Author
from app.domain.learnings.types import Learning


@dataclass(frozen=True)
class PromptSection:
    """Representa uma se√ß√£o estruturada do prompt principal."""
    title: str
    content: str

    def to_markdown(self) -> str:
        """Converte a se√ß√£o em markdown com t√≠tulo de segundo n√≠vel."""
        normalized_content = self.content.strip()
        if not normalized_content:
            normalized_content = "_Nenhum conte√∫do dispon√≠vel no momento._"
        return f"## {self.title}\n{normalized_content}"


class PromptRenderer:
    """Respons√°vel por renderizar partes din√¢micas do prompt em Markdown."""

    def __init__(
        self,
        summary_sentence_limit: int = 2,
        max_learning_preview_chars: int = 280,
    ) -> None:
        self.summary_sentence_limit = summary_sentence_limit
        self.max_learning_preview_chars = max_learning_preview_chars

    def render_system_instruction(
        self,
        instruction: AgentInstruction,
        template_version: str,
    ) -> str:
        """Constr√≥i o conte√∫do do system prompt."""
        base_persona = (
            "Voc√™ √© o Conselheiro Cultural oficial do Instituto. Sua miss√£o √© apoiar "
            "colaboradores a refletirem sobre dilemas cotidianos √† luz dos valores, princ√≠pios "
            "e aprendizados registrados nos artefatos culturais."
        )

        rules = [
            "Cite sempre as fontes na forma [Fonte X], mantendo X coerente com os artefatos fornecidos.",
            "Se n√£o houver contexto suficiente, admita limita√ß√µes e sugira pr√≥ximos passos.",
            "Mantenha tom acolhedor, curioso e convidativo √† reflex√£o (evite ordens diretas).",
            "Use Markdown avan√ßado (t√≠tulos, listas, blockquotes) quando isso tornar a resposta mais clara.",
            "Antes de entregar a resposta final, fa√ßa uma autoavalia√ß√£o silenciosa para verificar se todas as recomenda√ß√µes seguem as fontes e os aprendizados citados.",
        ]

        rules_block = "\n".join(f"- {item}" for item in rules)
        prompt_version = instruction.prompt_version or template_version

        return (
            f"{base_persona}\n\n"
            f"Vers√£o ativa do template: {prompt_version}\n"
            f"Atualizado em: {instruction.updated_at.isoformat()}\n\n"
            f"{instruction.content.strip()}\n\n"
            "Regras de atua√ß√£o:\n"
            f"{rules_block}"
        )

    def format_artifact_chunk(self, chunk: ArtifactChunk, index: int) -> str:
        """Formata um chunk de artefato com metadados e resumo."""
        metadata = chunk.metadata
        section_title = metadata.section_title if metadata and metadata.section_title else f"Trecho {index}"
        breadcrumbs = metadata.breadcrumbs if metadata else []
        breadcrumbs_text = f" ‚Ä∫ ".join(breadcrumbs) if breadcrumbs else ""
        content_type = metadata.content_type if metadata and metadata.content_type else "texto"
        token_count = metadata.token_count if metadata else "?"
        position = metadata.position if metadata else "?"

        summary = self._summarize_text(chunk.content)
        breadcrumb_line = f"- Breadcrumbs: {breadcrumbs_text}\n" if breadcrumbs_text else ""

        details = (
            f"- Tipo: {content_type}\n"
            f"- Chunk ID: {chunk.id}\n"
            f"- Posi√ß√£o: {position}\n"
            f"- Tokens (aprox.): {token_count}\n"
            f"{breadcrumb_line}"
            f"- Resumo: {summary}"
        ).strip()

        return (
            f"### Fonte {index} ‚Äî {section_title}\n"
            f"{details}\n\n"
            f"{chunk.content.strip()}"
        ).strip()

    def format_learning(self, learning: Learning, index: int) -> str:
        """Formata aprendizados priorizados com destaque visual."""
        preview = learning.content.strip()
        if len(preview) > self.max_learning_preview_chars:
            preview = f"{preview[:self.max_learning_preview_chars]}..."

        created_at = learning.created_at.isoformat() if isinstance(learning.created_at, datetime) else str(learning.created_at)

        return (
            f"üß† Insight Relevante #{index}\n"
            f"- Learning ID: {learning.id}\n"
            f"- Registrado em: {created_at}\n"
            f"> {preview}"
        ).strip()

    def render_conversation_history(self, conversation_history: Sequence[Message], limit: int) -> str:
        """Renderiza as mensagens recentes em formato cronol√≥gico."""
        if not conversation_history:
            return "_N√£o h√° hist√≥rico relevante registrado._"

        recent_messages = conversation_history[-limit:]
        lines: List[str] = []

        for message in recent_messages:
            role = "Usu√°rio" if message.author == Author.USER else "Agente"
            lines.append(f"- **{role}**: {message.content.strip()}")

        return "\n".join(lines)

    def compose_user_message(self, sections: Sequence[PromptSection]) -> str:
        """Combina m√∫ltiplas se√ß√µes em um √∫nico conte√∫do markdown."""
        rendered_sections = [section.to_markdown() for section in sections]
        return "\n\n".join(rendered_sections)

    def build_meta_instructions(self) -> str:
        """Retorna instru√ß√µes expl√≠citas para a etapa de metaprompt dentro do contexto."""
        return (
            "Antes de responder, sintetize mentalmente (sem escrever) os passos a seguir:\n"
            "1. Identificar quais fontes sustentam cada recomenda√ß√£o.\n"
            "2. Garantir que aprendizados destacados estejam conectados √† situa√ß√£o.\n"
            "3. Revisar o tom para manter postura de parceiro reflexivo.\n"
            "Quando concluir a resposta, confirme se todas as cita√ß√µes est√£o corretas. "
            "Se perceber inconsist√™ncias, ajuste a resposta antes de finaliz√°-la."
        )

    def _summarize_text(self, text: str) -> str:
        """Extrai um resumo simples com base nas primeiras frases."""
        sanitized = re.sub(r"\s+", " ", text.strip())
        if not sanitized:
            return "Conte√∫do indispon√≠vel."

        sentences = re.split(r"(?<=[.!?])\s+", sanitized)
        summary = " ".join(sentences[: self.summary_sentence_limit])
        return summary if summary else sanitized[:200]


@dataclass
class PromptTemplate:
    """Template principal que organiza o prompt em se√ß√µes e mensagens."""
    name: str = "professional_rag"
    version: str = "v1"
    max_artifacts: int = 5
    max_learnings: int = 3
    max_history_messages: int = 6
    renderer: PromptRenderer = field(default_factory=PromptRenderer)
    self_reflection_schema: str = field(
        default=(
            "Analise a resposta gerada e retorne um JSON com o formato:\n"
            "{\n"
            '  "revision_needed": true|false,\n'
            '  "issues": ["descri√ß√£o do problema"...],\n'
            '  "improvements": "texto curto com orienta√ß√µes para corrigir"\n'
            "}\n"
            "Considere ader√™ncia √†s fontes, alinhamento com aprendizados e tom adotado."
        )
    )

    def build_messages(
        self,
        instruction: AgentInstruction,
        artifacts: Sequence[ArtifactChunk],
        learnings: Sequence[Learning],
        conversation_history: Sequence[Message],
        user_query: str,
        few_shot_examples: Sequence[PromptExample] | None = None,
    ) -> List[dict]:
        """Constr√≥i a lista de mensagens para envio ao modelo Gemini."""
        system_content = self.renderer.render_system_instruction(instruction, self.version)

        artifact_entries = [
            self.renderer.format_artifact_chunk(chunk, index=index)
            for index, chunk in enumerate(artifacts[: self.max_artifacts], start=1)
        ]
        artifacts_section = PromptSection(
            title="Artefatos Relevantes",
            content="\n\n".join(artifact_entries) if artifact_entries else "_Nenhum artefato dispon√≠vel._",
        )

        learning_entries = [
            self.renderer.format_learning(learning, index=index)
            for index, learning in enumerate(learnings[: self.max_learnings], start=1)
        ]
        learnings_section = PromptSection(
            title="Aprendizados Priorizados",
            content="\n\n".join(learning_entries) if learning_entries else "_Nenhum aprendizado selecionado._",
        )

        history_section = PromptSection(
            title="Hist√≥rico Recente da Conversa",
            content=self.renderer.render_conversation_history(conversation_history, self.max_history_messages),
        )

        meta_section = PromptSection(
            title="Checklist de Meta-Avalia√ß√£o",
            content=self.renderer.build_meta_instructions(),
        )

        user_section = PromptSection(
            title="Pedido Atual do Usu√°rio",
            content=user_query.strip(),
        )

        user_content = self.renderer.compose_user_message(
            [artifacts_section, learnings_section, history_section, meta_section, user_section]
        )

        messages: List[dict] = [
            {"role": "system", "parts": [{"text": system_content}]},
        ]

        if few_shot_examples:
            for example in few_shot_examples:
                messages.extend(example.to_messages())

        messages.append({"role": "user", "parts": [{"text": user_content}]})
        return messages

    def build_self_reflection_prompt(
        self,
        user_query: str,
        draft_response: str,
        cited_artifacts: Sequence[ArtifactChunk],
        learnings: Sequence[Learning],
    ) -> str:
        """Constr√≥i o prompt que solicita autoavalia√ß√£o ap√≥s a primeira resposta."""
        artifacts_summary = "\n".join(
            f"- Fonte {index}: {self.renderer._summarize_text(chunk.content)}"
            for index, chunk in enumerate(cited_artifacts[: self.max_artifacts], start=1)
        )
        learnings_summary = "\n".join(
            f"- {self.renderer._summarize_text(learning.content)}"
            for learning in learnings[: self.max_learnings]
        )

        return (
            "Voc√™ agora atua como revisor cr√≠tico da resposta anterior do Conselheiro Cultural.\n"
            "Avalie se a resposta mant√©m ader√™ncia aos artefatos e aprendizados e se as cita√ß√µes est√£o corretas.\n\n"
            f"Pergunta original: {user_query.strip()}\n\n"
            "Resumo dos artefatos utilizados:\n"
            f"{artifacts_summary if artifacts_summary else '- Nenhum artefato informado.'}\n\n"
            "Resumo dos aprendizados selecionados:\n"
            f"{learnings_summary if learnings_summary else '- Nenhum aprendizado selecionado.'}\n\n"
            "Resposta proposta:\n"
            f"{draft_response.strip()}\n\n"
            f"{self.self_reflection_schema}"
        ).strip()

    def build_revision_messages(
        self,
        base_messages: Sequence[dict],
        draft_response: str,
        reflection_report: dict,
    ) -> List[dict]:
        """Gera nova sequ√™ncia de mensagens para solicitar revis√£o da resposta."""
        revised_messages = list(base_messages)
        revised_messages.append({"role": "model", "parts": [{"text": draft_response.strip()}]})

        reflection_summary = self._render_reflection_summary(reflection_report)
        revised_messages.append({"role": "user", "parts": [{"text": reflection_summary}]})
        return revised_messages

    def _render_reflection_summary(self, reflection_report: dict) -> str:
        """Transforma o relat√≥rio JSON em instru√ß√£o textual para revis√£o."""
        issues = reflection_report.get("issues") or []
        improvements = reflection_report.get("improvements") or ""
        issues_block = "\n".join(f"- {issue}" for issue in issues) if issues else "- Sem problemas listados explicitamente."

        return (
            "Com base na autoavalia√ß√£o anterior, revise a resposta para corrigir os pontos a seguir. "
            "Mantenha as regras originais e cite as fontes coerentemente. "
            "N√£o mencione esta revis√£o ao usu√°rio.\n\n"
            "Problemas identificados:\n"
            f"{issues_block}\n\n"
            "Orienta√ß√£o adicional:\n"
            f"{improvements if improvements else 'Ajuste apenas o necess√°rio para garantir ader√™ncia e tom adequado.'}"
        )

